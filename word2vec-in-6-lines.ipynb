{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Word2Vec in 6 lines of Python\n",
    "    \n",
    "    1  import requests\n",
    "    2  import gensim\n",
    "    3  url = 'http://www.gutenberg.org/cache/epub/1041/pg1041.txt'\n",
    "    4  text = requests.get(url).text\n",
    "    5  tokens = gensim.utils.simple_preprocess(t)\n",
    "    6  model = gensim.models.Word2Vec([tokens], min_count=3, size=100)\n",
    "    \n",
    "-----\n",
    "\n",
    "## Breakdown of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    '''input url, output text from page'''\n",
    "    r = requests.get(url).text\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch Shakespeare's sonnets\n",
    "text = fetch('http://www.gutenberg.org/cache/epub/1041/pg1041.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"own bright eyes,\\r\\n  Feed'st thy light's flame with self-substantial fuel,\\r\\n  Making a famine where abundance lies,\\r\\n  Thy self thy foe, to thy sweet self too cruel:\\r\\n  Thou that art now the world's fresh ornament,\\r\\n  And only herald to the gaudy spring,\\r\\n  Within thine own bud buriest thy content,\\r\\n  And tender churl mak'st waste in niggarding:\\r\\n    Pity the world, or else this glutton be,\\r\\n    To eat the world's due, by the grave and thee.\\r\\n\\r\\n  II\\r\\n\\r\\n  When forty winters shall besiege thy brow,\\r\\n  And dig deep trenches in thy beauty's field,\\r\\n  Thy youth's proud livery so gazed on now,\\r\\n  Will be a tatter'd weed of small worth held:\\r\\n  Then being asked, where all thy beauty lies,\\r\\n  Where all the treasure of thy lusty days;\\r\\n  To say, within thine own deep sunken eyes,\\r\\n  Were an all-eating shame, and thriftless praise.\\r\\n  How much more praise deserv'd thy beauty's use,\\r\\n  If thou couldst answer 'This fair child of mine\\r\\n  Shall sum my count, and make my old excuse,'\\r\\n  Proving his beauty by succession thine!\\r\\n    This were to be new made when thou art old,\\r\\n    And see thy blood warm when thou feel'st it cold.\\r\\n\\r\\n  III\\r\\n\\r\\n  Look in thy glass and tell the face thou viewest\\r\\n  Now is the time that face should form another;\\r\\n  Whose fresh repair if now thou not renewest,\\r\\n  Thou dost beguile the world, unbless some mother.\\r\\n  For where is she so fair whose unear'd womb\\r\\n  Disdains the tillage of thy husbandry?\\r\\n  Or who is he so fond will be the tomb,\\r\\n  Of his self-love to stop posterity?\\r\\n  Thou art thy mother's glass and she in thee\\r\\n  Calls back the lovely April of her prime;\\r\\n  So thou through windows of thine age shalt see,\\r\\n  Despite of wrinkles this thy golden time.\\r\\n    But if thou live, remember'd not to be,\\r\\n    Die single and thine image dies with thee.\\r\\n\\r\\n  IV\\r\\n\\r\\n  Unthrifty loveliness, why dost thou spend\\r\\n  Upon thy self thy beauty's legacy?\\r\\n  Nature's bequest gives nothing, but doth lend,\\r\\n  And being frank she lends to those are free:\\r\\n  Then, beaute\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1000:3001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "tokenized = gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20247"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in',\n",
       " u'single',\n",
       " u'life',\n",
       " u'ah',\n",
       " u'if',\n",
       " u'thou',\n",
       " u'issueless',\n",
       " u'shalt',\n",
       " u'hap',\n",
       " u'to',\n",
       " u'die',\n",
       " u'the',\n",
       " u'world',\n",
       " u'will',\n",
       " u'wail',\n",
       " u'thee',\n",
       " u'like',\n",
       " u'makeless',\n",
       " u'wife',\n",
       " u'the',\n",
       " u'world']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[1000:1021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the corpus, find most frequently occuring tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the frequency of each word in list\n",
    "def freq(tokens):\n",
    "    '''input list tokens, output frequency of each token'''\n",
    "    word_freq = [tokens.count(p) for p in tokens]\n",
    "    return zip(tokens, word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_list = freq(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20247"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'by', 118),\n",
       " (u'william', 4),\n",
       " (u'shakespeare', 9),\n",
       " (u'from', 97),\n",
       " (u'fairest', 5),\n",
       " (u'creatures', 2),\n",
       " (u'we', 24),\n",
       " (u'desire', 11),\n",
       " (u'increase', 4),\n",
       " (u'that', 338),\n",
       " (u'thereby', 2),\n",
       " (u'beauty', 70),\n",
       " (u'rose', 6),\n",
       " (u'might', 26),\n",
       " (u'never', 16),\n",
       " (u'die', 12),\n",
       " (u'but', 168),\n",
       " (u'as', 132),\n",
       " (u'the', 613),\n",
       " (u'riper', 2),\n",
       " (u'should', 46)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_list[100:121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 613),\n",
       " (u'and', 560),\n",
       " (u'to', 495),\n",
       " (u'of', 488),\n",
       " (u'in', 380),\n",
       " (u'my', 372),\n",
       " (u'that', 338),\n",
       " (u'thy', 281),\n",
       " (u'thou', 235),\n",
       " (u'with', 228),\n",
       " (u'for', 198),\n",
       " (u'love', 195),\n",
       " (u'is', 194),\n",
       " (u'not', 188),\n",
       " (u'you', 183),\n",
       " (u'but', 168),\n",
       " (u'me', 164),\n",
       " (u'thee', 162),\n",
       " (u'be', 160),\n",
       " (u'or', 157)]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the above method is okay, but its not sorted.\n",
    "\n",
    "# A more direct and easy route that \n",
    "# sorts by most frequently occuring in corpus,\n",
    "# uses the Counter method from the collections library.\n",
    "# And its much less code!\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(tokenized)\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model & start testing/analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec([tokenized], min_count=3, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99791601478479208"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09490858,  0.09576362, -0.03184532,  0.11927754, -0.0824075 ,\n",
       "        0.01422762, -0.04995862, -0.0985888 , -0.01279412,  0.09472705,\n",
       "        0.0349883 , -0.10929005, -0.04712148,  0.03875189, -0.01393333,\n",
       "        0.12059111,  0.05010965, -0.06369404,  0.01640897, -0.02410512,\n",
       "       -0.1487816 , -0.06996666,  0.07345931,  0.16995013, -0.01546913,\n",
       "        0.02747393, -0.06288467,  0.11962786, -0.06722205, -0.09957583,\n",
       "       -0.01633924,  0.16259933,  0.0771884 , -0.00798618,  0.02566331,\n",
       "        0.05760846,  0.00768478,  0.01273172, -0.00411591, -0.08337752,\n",
       "       -0.03925778,  0.05824149, -0.08774456, -0.04724528,  0.0063543 ,\n",
       "       -0.0654636 ,  0.03776062,  0.01148849, -0.04262467,  0.04399591,\n",
       "        0.00581395, -0.00190668, -0.03522281, -0.03243601, -0.03173142,\n",
       "        0.00098932, -0.00621357, -0.03562969, -0.15038134,  0.03011033,\n",
       "       -0.07072259,  0.04718942, -0.02941588, -0.03766086, -0.0211026 ,\n",
       "        0.00745422, -0.10056863,  0.06429652,  0.09747244,  0.00922819,\n",
       "       -0.04644604, -0.04708366, -0.0557828 ,  0.03024773, -0.12507953,\n",
       "       -0.0582265 , -0.07103738,  0.08434138, -0.01802492, -0.00597637,\n",
       "       -0.03007632,  0.09137771,  0.01612829, -0.02991691,  0.00052318,\n",
       "        0.11118764, -0.00022709, -0.09830189,  0.04242226,  0.06687947,\n",
       "       -0.04634641,  0.02420382, -0.00851286, -0.01405751, -0.010359  ,\n",
       "       -0.03897178,  0.06687138, -0.01603028,  0.00832439, -0.01616815], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec1 = model['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 1.0),\n",
       " (u'she', 0.9991060495376587),\n",
       " (u'of', 0.9990603923797607),\n",
       " (u'might', 0.9990493059158325),\n",
       " (u'it', 0.9990442395210266),\n",
       " (u'hate', 0.999043881893158),\n",
       " (u'still', 0.9990429282188416),\n",
       " (u'upon', 0.9990413188934326),\n",
       " (u'when', 0.999036967754364),\n",
       " (u'should', 0.9990242719650269),\n",
       " (u'fair', 0.9990196824073792),\n",
       " (u'you', 0.9990196228027344),\n",
       " (u'what', 0.9990196228027344),\n",
       " (u'yet', 0.999017059803009),\n",
       " (u'her', 0.9990150332450867)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(vec1, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'men', 0.9999999403953552),\n",
       " (u'thou', 0.999750554561615),\n",
       " (u'their', 0.9997481107711792),\n",
       " (u'and', 0.999745786190033),\n",
       " (u'of', 0.9997451305389404),\n",
       " (u'with', 0.9997432827949524),\n",
       " (u'as', 0.9997431039810181),\n",
       " (u'my', 0.9997419118881226),\n",
       " (u'the', 0.9997345209121704),\n",
       " (u'his', 0.9997340440750122),\n",
       " (u'heart', 0.9997338652610779),\n",
       " (u'me', 0.9997330904006958),\n",
       " (u'to', 0.9997327327728271),\n",
       " (u'all', 0.9997305274009705),\n",
       " (u'be', 0.9997302293777466)]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = model['men']\n",
    "model.wv.similar_by_vector(v, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'women', 0.9999999403953552),\n",
       " (u'gift', 0.9904922246932983),\n",
       " (u'mistress', 0.9904735684394836),\n",
       " (u'smell', 0.9904466271400452),\n",
       " (u'youth', 0.9904308319091797),\n",
       " (u'fire', 0.9903542995452881),\n",
       " (u'cheeks', 0.9902323484420776),\n",
       " (u'must', 0.9902098178863525),\n",
       " (u'away', 0.9901643991470337),\n",
       " (u'could', 0.9901115894317627),\n",
       " (u'worse', 0.9901033639907837),\n",
       " (u'poor', 0.9900951385498047),\n",
       " (u'we', 0.9900863766670227),\n",
       " (u'then', 0.9900714755058289),\n",
       " (u'there', 0.9900569915771484)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = model['women']\n",
    "model.wv.similar_by_vector(v, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'love', 1.0000001192092896),\n",
       " (u'and', 0.9999521374702454),\n",
       " (u'my', 0.9999481439590454),\n",
       " (u'in', 0.9999479651451111),\n",
       " (u'of', 0.9999454021453857),\n",
       " (u'so', 0.9999450445175171),\n",
       " (u'thy', 0.9999448657035828),\n",
       " (u'thou', 0.9999443888664246),\n",
       " (u'to', 0.9999439120292664),\n",
       " (u'that', 0.9999434947967529)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = model['love']\n",
    "model.wv.similar_by_vector(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
